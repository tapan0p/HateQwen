{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8878383,"sourceType":"datasetVersion","datasetId":5343848},{"sourceId":8904527,"sourceType":"datasetVersion","datasetId":5353544}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[torch] accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:44:37.993041Z","iopub.execute_input":"2024-07-08T14:44:37.993729Z","iopub.status.idle":"2024-07-08T14:45:02.908713Z","shell.execute_reply.started":"2024-07-08T14:44:37.993695Z","shell.execute_reply":"2024-07-08T14:45:02.907557Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.41.2)\nCollecting transformers[torch]\n  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.23.2)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\nDownloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate, transformers\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\nSuccessfully installed accelerate-0.32.1 transformers-4.42.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:45:11.784366Z","iopub.execute_input":"2024-07-08T14:45:11.785060Z","iopub.status.idle":"2024-07-08T14:45:24.538632Z","shell.execute_reply.started":"2024-07-08T14:45:11.785027Z","shell.execute_reply":"2024-07-08T14:45:24.537693Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.42.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import AutoTokenizer\n\n# Load your data\ndf = pd.read_csv('/kaggle/input/dynahate/DynaHate.csv')\ndf.reset_index(drop=True, inplace=True)\n# Encode the labels\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['label'])\n\n\n# Split the data and reset the indices\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\ntrain_texts.reset_index(drop=True, inplace=True)\ntest_texts.reset_index(drop=True, inplace=True)\ntrain_labels.reset_index(drop=True, inplace=True)\ntest_labels.reset_index(drop=True, inplace=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:45:28.102409Z","iopub.execute_input":"2024-07-08T14:45:28.103294Z","iopub.status.idle":"2024-07-08T14:45:33.442042Z","shell.execute_reply.started":"2024-07-08T14:45:28.103259Z","shell.execute_reply":"2024-07-08T14:45:33.441054Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.size","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:45:42.142819Z","iopub.execute_input":"2024-07-08T14:45:42.143191Z","iopub.status.idle":"2024-07-08T14:45:42.151498Z","shell.execute_reply.started":"2024-07-08T14:45:42.143162Z","shell.execute_reply":"2024-07-08T14:45:42.150487Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"534872"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom peft import get_peft_model, LoraConfig, PeftType\nimport torch\n\n\nmodel_name=\"Qwen/Qwen2-1.5B\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Tokenize the text\ntrain_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\ntest_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:45:48.090757Z","iopub.execute_input":"2024-07-08T14:45:48.091722Z","iopub.status.idle":"2024-07-08T14:46:07.086794Z","shell.execute_reply.started":"2024-07-08T14:45:48.091682Z","shell.execute_reply":"2024-07-08T14:46:07.085959Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-08 14:45:50.454290: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-08 14:45:50.454437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-08 14:45:50.589065: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea976f85a8e44a54b214908b944cb969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b36d338148d489e8bdc7e64341bc42f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f89b8d6b141440f895a5b6ee37142f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b2a761108a412394dba13e1c8df0fa"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass HateSpeechDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = HateSpeechDataset(train_encodings, train_labels)\ntest_dataset = HateSpeechDataset(test_encodings, test_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:46:33.974601Z","iopub.execute_input":"2024-07-08T14:46:33.975316Z","iopub.status.idle":"2024-07-08T14:46:33.984583Z","shell.execute_reply.started":"2024-07-08T14:46:33.975285Z","shell.execute_reply":"2024-07-08T14:46:33.983491Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load the pre-trained model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    peft_type=PeftType.LORA,\n    task_type=\"SEQ_CLS\",\n    r=2,  # rank of the low-rank adaptation matrices\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=['k_proj', 'v_proj'],\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:46:38.101155Z","iopub.execute_input":"2024-07-08T14:46:38.101972Z","iopub.status.idle":"2024-07-08T14:46:57.328240Z","shell.execute_reply.started":"2024-07-08T14:46:38.101942Z","shell.execute_reply":"2024-07-08T14:46:57.327453Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"302f9e91acb544988c40b8805543007d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60aaf8be24114c24aaedc6ed2bb96f5b"}},"metadata":{}},{"name":"stderr","text":"Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2-1.5B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.001,\n    logging_dir='./logs',\n    logging_steps=200,\n    eval_strategy=\"steps\",\n    gradient_accumulation_steps=8,\n    fp16=True,\n    save_strategy=\"no\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:47:03.906885Z","iopub.execute_input":"2024-07-08T14:47:03.907273Z","iopub.status.idle":"2024-07-08T14:47:03.964771Z","shell.execute_reply.started":"2024-07-08T14:47:03.907243Z","shell.execute_reply":"2024-07-08T14:47:03.963493Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:47:07.387664Z","iopub.execute_input":"2024-07-08T14:47:07.388039Z","iopub.status.idle":"2024-07-08T14:47:10.931588Z","shell.execute_reply.started":"2024-07-08T14:47:07.388010Z","shell.execute_reply":"2024-07-08T14:47:10.930776Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfor epoch in range(3):\n    trainer.train()\n    \n    #Evaluate the model\n    preds_output = trainer.predict(test_dataset)\n    preds = torch.argmax(torch.tensor(preds_output.predictions), axis=1)\n\n    # Generate classification report\n    print(classification_report(test_labels, preds, target_names=le.classes_))","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:47:18.800408Z","iopub.execute_input":"2024-07-08T14:47:18.800847Z","iopub.status.idle":"2024-07-08T18:58:43.719529Z","shell.execute_reply.started":"2024-07-08T14:47:18.800816Z","shell.execute_reply":"2024-07-08T18:58:43.718458Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240708_144726-5nkfyu8f</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mahatatapan2021-Indian%20Institute%20of%20Technology/huggingface/runs/5nkfyu8f' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/mahatatapan2021-Indian%20Institute%20of%20Technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mahatatapan2021-Indian%20Institute%20of%20Technology/huggingface' target=\"_blank\">https://wandb.ai/mahatatapan2021-Indian%20Institute%20of%20Technology/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mahatatapan2021-Indian%20Institute%20of%20Technology/huggingface/runs/5nkfyu8f' target=\"_blank\">https://wandb.ai/mahatatapan2021-Indian%20Institute%20of%20Technology/huggingface/runs/5nkfyu8f</a>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='514' max='514' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [514/514 1:15:40, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.436400</td>\n      <td>1.046006</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.856100</td>\n      <td>0.692631</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        hate       0.72      0.77      0.74      4376\n     nothate       0.71      0.66      0.68      3853\n\n    accuracy                           0.72      8229\n   macro avg       0.72      0.71      0.71      8229\nweighted avg       0.72      0.72      0.71      8229\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='514' max='514' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [514/514 1:15:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.557400</td>\n      <td>0.518949</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.490500</td>\n      <td>0.443459</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        hate       0.81      0.83      0.82      4376\n     nothate       0.80      0.78      0.79      3853\n\n    accuracy                           0.81      8229\n   macro avg       0.81      0.81      0.81      8229\nweighted avg       0.81      0.81      0.81      8229\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='514' max='514' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [514/514 1:15:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.416800</td>\n      <td>0.394313</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.394800</td>\n      <td>0.376943</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        hate       0.86      0.83      0.85      4376\n     nothate       0.82      0.84      0.83      3853\n\n    accuracy                           0.84      8229\n   macro avg       0.84      0.84      0.84      8229\nweighted avg       0.84      0.84      0.84      8229\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Get predictions\npreds_output = trainer.predict(test_dataset)\npreds = torch.argmax(torch.tensor(preds_output.predictions), axis=1)\n\n# Generate classification report\nprint(classification_report(test_labels, preds, target_names=le.classes_))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(\"Tapan101/hateQwen\", check_pr=True)\ntokenizer.push_to_hub(\"Tapan101/hateQwen\", check_pr=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
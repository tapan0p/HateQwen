{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8878383,"sourceType":"datasetVersion","datasetId":5343848}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[torch] accelerate -U","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:09:28.393761Z","iopub.execute_input":"2024-07-06T16:09:28.394462Z","iopub.status.idle":"2024-07-06T16:09:53.398408Z","shell.execute_reply.started":"2024-07-06T16:09:28.394429Z","shell.execute_reply":"2024-07-06T16:09:53.397305Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.41.2)\nCollecting transformers[torch]\n  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.23.2)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\nDownloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate, transformers\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\nSuccessfully installed accelerate-0.32.1 transformers-4.42.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:10:42.362849Z","iopub.execute_input":"2024-07-06T16:10:42.363231Z","iopub.status.idle":"2024-07-06T16:10:55.132638Z","shell.execute_reply.started":"2024-07-06T16:10:42.363199Z","shell.execute_reply":"2024-07-06T16:10:55.131523Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.42.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import AutoTokenizer\n\n# Load your data\ndf = pd.read_csv('/kaggle/input/dynahate/DynaHate.csv')\ndf.reset_index(drop=True, inplace=True)\n# Encode the labels\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['label'])\n\n\n# Split the data and reset the indices\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\ntrain_texts.reset_index(drop=True, inplace=True)\ntest_texts.reset_index(drop=True, inplace=True)\ntrain_labels.reset_index(drop=True, inplace=True)\ntest_labels.reset_index(drop=True, inplace=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:11:27.756037Z","iopub.execute_input":"2024-07-06T16:11:27.756998Z","iopub.status.idle":"2024-07-06T16:11:33.029837Z","shell.execute_reply.started":"2024-07-06T16:11:27.756959Z","shell.execute_reply":"2024-07-06T16:11:33.029047Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.size","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:11:37.965723Z","iopub.execute_input":"2024-07-06T16:11:37.966526Z","iopub.status.idle":"2024-07-06T16:11:37.974184Z","shell.execute_reply.started":"2024-07-06T16:11:37.966492Z","shell.execute_reply":"2024-07-06T16:11:37.973347Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"534872"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom peft import get_peft_model, LoraConfig, PeftType\nimport torch\n\n\nmodel_name=\"TinyLlama/TinyLlama_v1.1\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Tokenize the text\ntrain_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\ntest_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:11:41.702702Z","iopub.execute_input":"2024-07-06T16:11:41.703051Z","iopub.status.idle":"2024-07-06T16:12:01.690661Z","shell.execute_reply.started":"2024-07-06T16:11:41.703023Z","shell.execute_reply":"2024-07-06T16:12:01.689625Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-06 16:11:44.041111: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-06 16:11:44.041251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-06 16:11:44.178914: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233ebf7546bb4d939f3db289d4a30c12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7768bacf1b7a437d84c577c93deea08b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eb39ed015af422589d23bbeac957a8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"619fa6b678f04694a9f2aa2b640a29f4"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass HateSpeechDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = HateSpeechDataset(train_encodings, train_labels)\ntest_dataset = HateSpeechDataset(test_encodings, test_labels)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:12:13.126801Z","iopub.execute_input":"2024-07-06T16:12:13.127814Z","iopub.status.idle":"2024-07-06T16:12:13.134709Z","shell.execute_reply.started":"2024-07-06T16:12:13.127781Z","shell.execute_reply":"2024-07-06T16:12:13.133754Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Load the pre-trained model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    peft_type=PeftType.LORA,\n    task_type=\"SEQ_CLS\",\n    r=2,  # rank of the low-rank adaptation matrices\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=['k_proj', 'v_proj'],\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:12:16.562777Z","iopub.execute_input":"2024-07-06T16:12:16.563623Z","iopub.status.idle":"2024-07-06T16:14:46.783389Z","shell.execute_reply.started":"2024-07-06T16:12:16.563590Z","shell.execute_reply":"2024-07-06T16:14:46.782401Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dd68e5ae7dc4cbe8fb5a654011d8a37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a26b15063ccf43ec9ff2ab28db434c98"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of LlamaForSequenceClassification were not initialized from the model checkpoint at TinyLlama/TinyLlama_v1.1 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.001,\n    logging_dir='./logs',\n    logging_steps=200,\n    eval_strategy=\"steps\",\n    save_steps=600,\n    gradient_accumulation_steps=8,\n    fp16=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:14:48.769786Z","iopub.execute_input":"2024-07-06T16:14:48.770477Z","iopub.status.idle":"2024-07-06T16:14:48.823806Z","shell.execute_reply.started":"2024-07-06T16:14:48.770445Z","shell.execute_reply":"2024-07-06T16:14:48.823045Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T16:14:54.065638Z","iopub.execute_input":"2024-07-06T16:14:54.066570Z","iopub.status.idle":"2024-07-06T19:13:15.666034Z","shell.execute_reply.started":"2024-07-06T16:14:54.066530Z","shell.execute_reply":"2024-07-06T19:13:15.664769Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240706_161504-yp2w7vmp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mahatatapan2000-Indian%20Institute%20of%20Technology/huggingface/runs/yp2w7vmp' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/mahatatapan2000-Indian%20Institute%20of%20Technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mahatatapan2000-Indian%20Institute%20of%20Technology/huggingface' target=\"_blank\">https://wandb.ai/mahatatapan2000-Indian%20Institute%20of%20Technology/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mahatatapan2000-Indian%20Institute%20of%20Technology/huggingface/runs/yp2w7vmp' target=\"_blank\">https://wandb.ai/mahatatapan2000-Indian%20Institute%20of%20Technology/huggingface/runs/yp2w7vmp</a>"},"metadata":{}},{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1542' max='1542' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1542/1542 2:57:46, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.901900</td>\n      <td>0.831965</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.801000</td>\n      <td>0.758250</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.746900</td>\n      <td>0.713585</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.701000</td>\n      <td>0.689121</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.689700</td>\n      <td>0.676775</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.670100</td>\n      <td>0.668264</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.669400</td>\n      <td>0.664785</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1542, training_loss=0.7332807904551465, metrics={'train_runtime': 10698.2365, 'train_samples_per_second': 9.23, 'train_steps_per_second': 0.144, 'total_flos': 7.344957477303091e+16, 'train_loss': 0.7332807904551465, 'epoch': 2.997812879708384})"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Get predictions\npreds_output = trainer.predict(test_dataset)\npreds = torch.argmax(torch.tensor(preds_output.predictions), axis=1)\n\n# Generate classification report\nprint(classification_report(test_labels, preds, target_names=le.classes_))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T19:14:33.871342Z","iopub.execute_input":"2024-07-06T19:14:33.872016Z","iopub.status.idle":"2024-07-06T19:20:13.612678Z","shell.execute_reply.started":"2024-07-06T19:14:33.871985Z","shell.execute_reply":"2024-07-06T19:20:13.611637Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        hate       0.63      0.68      0.66      4376\n     nothate       0.60      0.55      0.57      3853\n\n    accuracy                           0.62      8229\n   macro avg       0.62      0.61      0.61      8229\nweighted avg       0.62      0.62      0.62      8229\n\n","output_type":"stream"}]}]}